{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lasagne\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import random\n",
    "import time\n",
    "import connect_four as cccc\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#initialization\n",
    "\n",
    "generations = []\n",
    "\n",
    "hidden_units = 16\n",
    "\n",
    "\n",
    "\n",
    "value_in = lasagne.layers.InputLayer(shape=(None,42))\n",
    "\n",
    "value_hid1 = lasagne.layers.DenseLayer(value_in, num_units=hidden_units,W=lasagne.init.GlorotUniform(),\n",
    "                                          nonlinearity = lasagne.nonlinearities.tanh)\n",
    "value_hid2 = lasagne.layers.DenseLayer(value_hid1, num_units=hidden_units,W=lasagne.init.GlorotUniform(),\n",
    "                                          nonlinearity = lasagne.nonlinearities.tanh)\n",
    "\n",
    "value_hid3 = lasagne.layers.DenseLayer(value_hid2, num_units=hidden_units,W=lasagne.init.GlorotUniform(),\n",
    "                                          nonlinearity = lasagne.nonlinearities.tanh)\n",
    "\n",
    "value_drop1 = lasagne.layers.DropoutLayer(value_hid3,p=0.5)\n",
    "\n",
    "value_out = lasagne.layers.DenseLayer(value_drop1,\n",
    "                                  num_units=1, nonlinearity = lasagne.nonlinearities.tanh)\n",
    "\n",
    "\n",
    "\n",
    "# policy_in = lasagne.layers.InputLayer(shape=(1,9))\n",
    "\n",
    "# #l_drop1 = lasagne.layers.DropoutLayer(l_shape,p=0.2)\n",
    "\n",
    "# policy_hid1 = lasagne.layers.DenseLayer(policy_in, num_units=hidden_units,W=lasagne.init.GlorotUniform(),\n",
    "#                                           nonlinearity = lasagne.nonlinearities.rectify)\n",
    "\n",
    "# #l_drop2 = lasagne.layers.DropoutLayer(l_hid1,p=0.5)\n",
    "\n",
    "# policy_out = lasagne.layers.DenseLayer(policy_hid1,\n",
    "#                                   num_units=9,\n",
    "#                                   nonlinearity=lasagne.nonlinearities.softmax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def value_move(board,active_turn,output_fun,exploration = 0):\n",
    "    board = board.reshape((1,42))\n",
    "    X_sym = theano.tensor.matrix()\n",
    "    y_sym = theano.tensor.ivector()\n",
    "\n",
    "    dummy_board = active_turn * board[:]\n",
    "    options = cccc.available_moves(dummy_board)\n",
    "    \n",
    "    \n",
    "    if exploration > random.random():\n",
    "        move = random.choice(options) \n",
    "    else:\n",
    "        move_values = np.zeros(42)\n",
    "        for move in options:\n",
    "            dummy_board = active_turn * board[:]\n",
    "            dummy_board[0][move] = 1\n",
    "            move_values[move] = -1 * output_fun(-1* dummy_board)\n",
    "        \n",
    "\n",
    "        available_move_values = np.array([move_values[move] for move in options])\n",
    "        \n",
    "        move = options[available_move_values.argmax(-1)]\n",
    "    return move + 1\n",
    "\n",
    "\n",
    "class nn_ai:\n",
    "    \n",
    "    def __init__(self,output_fun, net = 'value',exploration = 0):\n",
    "        self.output_fun = output_fun\n",
    "        self.exploration = exploration\n",
    "        self.net = net\n",
    "    \n",
    "    def make_move(self,board,active_turn):\n",
    "#         if self.net == 'policy':\n",
    "#             move = policy_move(board,active_turn,self.output_fun,self.exploration)\n",
    "        if self.net == 'value':\n",
    "            move = value_move(board,active_turn,self.output_fun,self.exploration)\n",
    "        return move\n",
    "    \n",
    "def alpha_beta_move(board,active_turn,depth,alpha = 2):\n",
    "    swap_dict = {1:-1,-1:1}\n",
    "    dummy_board = np.zeros((6,7))\n",
    "    dummy_board[:] = board[:]\n",
    "    options = cccc.available_moves(board)\n",
    "    random.shuffle(options)\n",
    "    if len(options) == 1:\n",
    "        dummy_board[np.where(dummy_board[:,options[0]]==0)[0][-1],options[0]] = active_turn\n",
    "        if cccc.winner(dummy_board):\n",
    "            return (1,options[0]+1)\n",
    "        else:\n",
    "            return (0,options[0]+1)\n",
    "    if depth ==0:\n",
    "        return (0, options[np.random.randint(len(options))]+1)\n",
    "\n",
    "    best_value = -2\n",
    "    candidate_move = None\n",
    "    for x in options:\n",
    "        height = np.where(dummy_board[:,x]==0)[0][-1]\n",
    "        dummy_board[height,x] = active_turn\n",
    "        if cccc.winner(dummy_board):\n",
    "            return (1, x+1)\n",
    "        (opp_value,opp_move) = alpha_beta_move(dummy_board,swap_dict[active_turn],depth-1,-best_value)\n",
    "        if -opp_value > best_value:\n",
    "            candidate_move = x+1\n",
    "            best_value = -opp_value\n",
    "        if -opp_value >= alpha:\n",
    "            #print (options, x, best_value, alpha)\n",
    "            break\n",
    "        dummy_board[height,x] = 0\n",
    "\n",
    "    return (best_value, candidate_move)\n",
    "\n",
    "class alpha_beta:\n",
    "    def __init__(self,depth):\n",
    "        self.depth = depth\n",
    "    def make_move(self,board,active_turn):\n",
    "        #print (board,active_turn,self.depth)\n",
    "        return alpha_beta_move(board,active_turn,self.depth)[1]\n",
    "\n",
    "def get_max_future(future_board,value_fun):\n",
    "    options = cccc.available_moves(future_board)\n",
    "    dummy_board = np.copy(future_board)\n",
    "    move_values = np.zeros(7)\n",
    "    for move in options:\n",
    "        dummy_board = np.copy(future_board)\n",
    "        dummy_board[np.where(dummy_board[:,move]==0)[0][-1],move] = -1\n",
    "        # dummy_board = dummy_board.reshape(1,42)\n",
    "        if cccc.winner(dummy_board):\n",
    "            move_values[move] = cccc.winner(dummy_board)\n",
    "        else:\n",
    "            reshapable = np.copy(dummy_board)\n",
    "            reshapable = reshapable.reshape(1,42)\n",
    "            move_values[move] = value_fun(reshapable)\n",
    "    \n",
    "    available_move_values = np.array([move_values[move] for move in options])\n",
    "    dummy_board = np.copy(future_board)\n",
    "    options_index = np.argmin(available_move_values)\n",
    "    dummy_board[np.where(dummy_board[:,options[options_index]]==0)[0][-1],options[options_index]] = -1\n",
    "    return np.amin(available_move_values), dummy_board\n",
    "\n",
    "def get_inputs(log):\n",
    "    boards = []\n",
    "    piece = 1\n",
    "    board = np.zeros((6,7))\n",
    "    boards.append(np.copy(board))\n",
    "    for move in log:\n",
    "        board[np.where(board[:,move-1]==0)[0][-1],move-1] = piece\n",
    "        piece = -piece\n",
    "        boards.append(np.copy(board))\n",
    "    return boards\n",
    "\n",
    "\n",
    "\n",
    "def game_over(board):\n",
    "    return cccc.winner(board) or cccc.is_full(board)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_sym = T.matrix()\n",
    "y_sym = T.matrix()\n",
    "\n",
    "input_history = []\n",
    "output_history = []\n",
    "results = {}\n",
    "move_history = []\n",
    "output = lasagne.layers.get_output(value_out,X_sym)\n",
    "output_det = lasagne.layers.get_output(value_out,X_sym,deterministic=True)\n",
    "value_fun = theano.function([X_sym],output)\n",
    "value_fun_det = theano.function([X_sym],output_det)\n",
    "params = lasagne.layers.get_all_params(value_out)\n",
    "objective = T.mean(lasagne.objectives.squared_error(output,y_sym))\n",
    "grad = T.grad(objective,params)\n",
    "\n",
    "#flush training sets\n",
    "input_history = []\n",
    "output_history = []\n",
    "results = {}\n",
    "move_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "batches_per_step = 500\n",
    "training_per_step =100\n",
    "train_duration = 20\n",
    "exploration = 1\n",
    "exploration_min = 0.05\n",
    "exploration_max = 0.95\n",
    "future_discount = 0.5\n",
    "minimax_str = 0\n",
    "validation_str = 2\n",
    "monte_carlo_duration = 0\n",
    "print_freq = batches_per_step-1\n",
    "valid_freq = 5\n",
    "learning_speed = 0.01\n",
    "updates = lasagne.updates.nesterov_momentum(grad, params, learning_rate=learning_speed,momentum = 0.9)\n",
    "#updates = lasagne.updates.sgd(grad, params, learning_rate=learning_speed)\n",
    "train_net = theano.function([X_sym, y_sym], objective, updates=updates)\n",
    "temporal_index = 0.8\n",
    "objective_total = 1000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t0 = time.clock()\n",
    "epoch_objective_list = []\n",
    "\n",
    "param_values = lasagne.layers.get_all_param_values(value_out)\n",
    "    \n",
    "minimum_data = sum([param_values[i].size for i,_ in enumerate(param_values)])\n",
    "while(len(output_history) < 2 * minimum_data):\n",
    "    exploration = min(exploration_max ,\n",
    "                  exploration*0.99 + 0.01 *(min(exploration_max-exploration_min,objective_total) + exploration_min))\n",
    "    future_discount = future_discount*0.99 + 0.01*(1 - min(1,objective_total))\n",
    "    # future_discount = 1\n",
    "    result_X = cccc.play(nn_ai(value_fun,net = 'value',exploration = exploration),alpha_beta(minimax_str))\n",
    "    board_list =get_inputs(result_X.log)\n",
    "    game_length = len(result_X.log)\n",
    "    input_list = [board_list[2*i] for i in range((game_length+1)/2)]\n",
    "    output_list = [board_list[2*i+1] for i in range((game_length+1)/2)]\n",
    "    move_list = [result_X.log[2*i] for i in range((game_length+1)/2)]\n",
    "\n",
    "\n",
    "\n",
    "    input_history = input_history+ input_list\n",
    "    output_history = output_history+output_list\n",
    "    move_history = move_history+move_list\n",
    "    #reward_history = reward_history + reward_list\n",
    "\n",
    "\n",
    "    result_O = cccc.play(alpha_beta(minimax_str),nn_ai(value_fun,net = 'value', exploration = exploration))\n",
    "    board_list = get_inputs(result_O.log)\n",
    "    game_length = len(result_O.log)\n",
    "    input_list = [-1*board_list[2*i+1] for i in range(game_length/2)]\n",
    "    output_list = [-1*board_list[2*i+2] for i in range(game_length/2)]\n",
    "    move_list = [result_O.log[2*i+1] for i in range(game_length/2)]\n",
    "for epoch in range(train_duration):\n",
    "    \n",
    "    t1 = time.clock()\n",
    "     \n",
    "    if len(input_history) > minimum_data:\n",
    "        target_history = np.zeros(len(output_history))\n",
    "        print 'Creating Targets for {} data points'.format(len(output_history))\n",
    "        print '\\n'\n",
    "        t3 = time.clock()\n",
    "        for i,item in enumerate(output_history):\n",
    "            output_state = np.copy(output_history[i])\n",
    "            if cccc.winner(output_state) or cccc.is_full(output_state):\n",
    "                target_history[i] = cccc.winner(output_state)\n",
    "            else:\n",
    "            #minus because the future term is in terms of the valuation for the player, and we need a target for the \n",
    "            #opponent\n",
    "            #    targets[i] = (1-future_discount) * reward_state + future_discount * get_max_future(\n",
    "            #output_state,value_fun)\n",
    "            #targets = np.array(targets).reshape(BATCH_SIZE,1)\n",
    "\n",
    "                #temporal difference method\n",
    "                target_history[i]= 0\n",
    "                current_state = np.copy(output_state)\n",
    "\n",
    "                depth = 0\n",
    "                player = 1\n",
    "\n",
    "                while not game_over(np.copy(current_state)):\n",
    "                    current_value , next_state= get_max_future(current_state,value_fun)\n",
    "\n",
    "                    #get_max_future calculates the min future for other player moving next\n",
    "                    # so the negative player is going to want to reverse it\n",
    "                    current_value = player * current_value\n",
    "                    depth +=1\n",
    "                    target_history[i] += (temporal_index**(depth-1))* (1-temporal_index) *current_value\n",
    "                    current_state = -1 * np.copy(next_state)\n",
    "                    player *= -1\n",
    "\n",
    "                target_history[i] += temporal_index**depth * player* cccc.winner(current_state)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #we reverse the target because we are evaulating the opponenet's position\n",
    "            target_history[i] = -1 * target_history[i]\n",
    "        print 'Time to create targets: {}s'.format(time.clock()-t3)\n",
    "        print '\\n'\n",
    "        for j in range(batches_per_step):\n",
    "            t2=time.clock()\n",
    "            targets = np.zeros((BATCH_SIZE,1))\n",
    "            training = np.zeros((BATCH_SIZE,42))\n",
    "            index_pool = range(len(input_history))\n",
    "            random.shuffle(index_pool)\n",
    "            objectives = []\n",
    "            \n",
    "            while len(index_pool) > BATCH_SIZE:\n",
    "                sample_indices = [index_pool.pop() for _ in xrange(BATCH_SIZE)]\n",
    "                #Should try to use generators\n",
    "                \n",
    "\n",
    "                \n",
    "            \n",
    "            \n",
    "                for k in range(BATCH_SIZE):\n",
    "                    #train it on output_history evaluated by the opponent\n",
    "                    training[k] = (-1*np.copy(output_history[sample_indices[k]])).reshape(1,42)\n",
    "                    targets[k] = target_history[sample_indices[k]]\n",
    "                    #training[i] = training[i].reshape(1,9)\n",
    "\n",
    "        #               reward_state = reward_history[sample_indices[i]]\n",
    "                    #move_state = move_history[sample_indices[i]]\n",
    "                    #reward_state= -1 * results[tuple(-1 * output_state)]['result'] / float(\n",
    "                    #                   results[tuple(-1 * output_state)]['plays'])\n",
    "\n",
    "                    #needs to evaluate to the result of the opponent's board\n",
    "                    #If move is a winning move, reward_state will evaluate to 1. Since the board is reversed, this is the\n",
    "                    #reverse evaluation\n",
    "\n",
    "\n",
    "\n",
    "                objectives.append(train_net(training,targets))\n",
    "                #break to only do 1 batch per run.\n",
    "            objective_total = np.mean(objectives)\n",
    "            if j%print_freq ==0:\n",
    "                print (('Epoch {:5d}, Pass number {}, objective: {:0.5f}, exploration: {:0.2f}, '+\n",
    "                       'step duration: {:1.3f}s').format(\n",
    "                       epoch,j+1,float(objective_total),exploration,time.clock()-t2))\n",
    "                \n",
    "                \n",
    "            epoch_objective_list.append([epoch,j+1,objective_total])\n",
    "            \n",
    "            \n",
    "        print 'epoch duration {:2.2f}s'.format(time.clock()-t1)\n",
    "        if epoch%valid_freq==0:\n",
    "            print ' '\n",
    "            print ' '\n",
    "            test_result = {'wins':0,'ties':0,'losses':0}\n",
    "            for j in range(100):\n",
    "                result = cccc.play(nn_ai(value_fun_det,'value'),alpha_beta(validation_str))\n",
    "                if result.winner ==1:\n",
    "                    test_result['wins'] +=1\n",
    "                if result.winner == 0:\n",
    "                    test_result['ties'] +=1\n",
    "                if result.winner == -1:\n",
    "                    test_result['losses'] +=1\n",
    "\n",
    "\n",
    "            print 'As X, neural network has a score of {:3d}-{:3d}-{:3d} vs {}-depth minimax'.format(test_result['wins'],\n",
    "                                                                   test_result['ties'],test_result['losses'],validation_str)\n",
    "    \n",
    "            test_result = {'wins':0,'ties':0,'losses':0}\n",
    "\n",
    "            for j in range(100):\n",
    "                result = cccc.play(alpha_beta(validation_str),nn_ai(value_fun_det,'value'))\n",
    "                if result.winner ==1:\n",
    "                    test_result['losses'] +=1\n",
    "                if result.winner == 0:\n",
    "                    test_result['ties'] +=1\n",
    "                if result.winner == -1:\n",
    "                    test_result['wins'] +=1\n",
    "\n",
    "            print 'As O, neural network has a score of {:3d}-{:3d}-{:3d} vs {}-depth minimax'.format(test_result['wins'],\n",
    "                                                                test_result['ties'],test_result['losses'],validation_str)\n",
    "            print ' '\n",
    "            print 'elapsed time: {:3.3f}s'.format(time.clock()-t0)\n",
    "            print ' '\n",
    "    else:\n",
    "        if epoch%print_freq==0:\n",
    "            print ('Learning step {:5d}, training size {:4d}, step duration: {:1.3f}s'.format(\n",
    "                    epoch,len(input_history),time.clock()-t1))\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "#for epoch in range(10):\n",
    "\n",
    "        \n",
    "#    t_epoch = time.clock()\n",
    "\n",
    "#    result_list = [tourney(output) for i in range(BATCH_SIZE)]\n",
    "#    fitness_list = [fitness(result) for result in result_list]\n",
    "    \n",
    "#    score= f_train(fitness_list)\n",
    "       \n",
    "#    t1=time.clock()-t_epoch\n",
    "    \n",
    "#    print('Epoch {}, duration {:.01f} seconds'.format(\n",
    "#            epoch+1, t1))\n",
    "#    print('Record is: {}-{}-{} with a score of {}'.format(sum([result['wins'] for result in result_list]),\n",
    "#                                                          sum([result['ties'] for result in result_list]),\n",
    "#                                                          sum([result['losses'] for result in result_list]),\n",
    "#                                                          sum([fitness for fitness in fitness_list])))\n",
    "#    print('mean score is {}'.format(score))\n",
    "print('total time for neural network is {:.01f} seconds'.format(time.clock()-t0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('TD_connect_four.txt','w')\n",
    "\n",
    "for item in epoch_objective_list:\n",
    "    print>>f , item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_training = pd.read_csv('connect-4.csv',header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_set = []\n",
    "targets_set = []\n",
    "for index,row in raw_training.iterrows():\n",
    "    input_dict = {'b':0,'x':1,'o':-1}\n",
    "    game_state_dict = {'win':1,'draw':0,'loss':-1}\n",
    "    game_state = game_state_dict[row[42]]\n",
    "    row = row[:-1].apply(lambda x: input_dict[x])\n",
    "    board = np.flipud(np.reshape(np.array(row),(6,7),order = 'F'))\n",
    "    if np.sum(board)==1:\n",
    "        board = -1 * np.copy(board)\n",
    "    \n",
    "    training_set.append(np.copy(board).reshape(1,42))\n",
    "    targets_set.append(game_state)\n",
    "\n",
    "dummy_list = list(zip(training_set,targets_set))\n",
    "random.shuffle(dummy_list)\n",
    "training_set,targets_set = zip(*dummy_list)\n",
    "\n",
    "val_training_set = training_set[:len(training_set)/2]\n",
    "val_targets_set = targets_set[:len(targets_set)/2]\n",
    "training_set = training_set[len(training_set)/2:]\n",
    "targets_set = targets_set[len(targets_set)/2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_sym = T.matrix()\n",
    "y_sym = T.matrix()\n",
    "\n",
    "output = lasagne.layers.get_output(value_out,X_sym)\n",
    "output_det = lasagne.layers.get_output(value_out,X_sym,deterministic=True)\n",
    "value_fun = theano.function([X_sym],output)\n",
    "value_fun_det = theano.function([X_sym],output_det)\n",
    "params = lasagne.layers.get_all_params(value_out)\n",
    "objective = T.mean(lasagne.objectives.squared_error(output,y_sym))\n",
    "grad = T.grad(objective,params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "train_duration = 500\n",
    "minimax_str = 0\n",
    "validation_str = 2\n",
    "valid_freq = 100\n",
    "learning_speed = 0.01\n",
    "updates = lasagne.updates.nesterov_momentum(grad, params, learning_rate=learning_speed,momentum = 0.9)\n",
    "#updates = lasagne.updates.sgd(grad, params, learning_rate=learning_speed)\n",
    "train_net = theano.function([X_sym, y_sym], objective, updates=updates)\n",
    "val_net = theano.function([X_sym, y_sym], objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t0 = time.clock()\n",
    "\n",
    "epoch_objective_list = []\n",
    "\n",
    "for epoch in range(train_duration):\n",
    "\n",
    "\n",
    "    t2=time.clock()\n",
    "\n",
    "    objectives = []\n",
    "    index_pool = range(len(targets_set))\n",
    "    random.shuffle(index_pool)\n",
    "    while len(index_pool) > BATCH_SIZE:\n",
    "        \n",
    "        sample_indices = [index_pool.pop() for _ in xrange(BATCH_SIZE)]\n",
    "\n",
    "        targets = np.zeros((BATCH_SIZE,1))\n",
    "        training = np.zeros((BATCH_SIZE,42))\n",
    "        for k in range(BATCH_SIZE):\n",
    "            training[k] = np.copy(training_set[sample_indices[k]])\n",
    "            targets[k] = targets_set[sample_indices[k]]\n",
    "\n",
    "        objectives.append(train_net(training,targets))\n",
    "        #break to only do 1 batch per run.\n",
    "    objective_total = np.mean(objectives)\n",
    "\n",
    "\n",
    "    objectives = []\n",
    "    index_pool = range(len(val_targets_set))\n",
    "    random.shuffle(index_pool)\n",
    "    while len(index_pool) > BATCH_SIZE:\n",
    "        \n",
    "        sample_indices = [index_pool.pop() for _ in xrange(BATCH_SIZE)]\n",
    "\n",
    "        targets = np.zeros((BATCH_SIZE,1))\n",
    "        training = np.zeros((BATCH_SIZE,42))\n",
    "        for k in range(BATCH_SIZE):\n",
    "            training[k] = np.copy(training_set[sample_indices[k]])\n",
    "            targets[k] = targets_set[sample_indices[k]]\n",
    "\n",
    "        objectives.append(val_net(training,targets))\n",
    "        #break to only do 1 batch per run.\n",
    "    val_total = np.mean(objectives)\n",
    "\n",
    "    print (('Epoch {:5d},objective: {:0.5f}, validation: {:0.5f}, step duration: {:1.3f}s').format(\n",
    "            epoch,float(objective_total),float(val_total),time.clock()-t2))\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    epoch_objective_list.append([epoch,objective_total,val_total])\n",
    "\n",
    "    # print 'epoch duration {:2.2f}s'.format(time.clock()-t2)\n",
    "    if epoch%valid_freq==0:\n",
    "        print ' '\n",
    "        print ' '\n",
    "        test_result = {'wins':0,'ties':0,'losses':0}\n",
    "        for j in range(100):\n",
    "            result = cccc.play(nn_ai(value_fun_det,'value'),alpha_beta(validation_str))\n",
    "            if result.winner ==1:\n",
    "                test_result['wins'] +=1\n",
    "            if result.winner == 0:\n",
    "                test_result['ties'] +=1\n",
    "            if result.winner == -1:\n",
    "                test_result['losses'] +=1\n",
    "\n",
    "\n",
    "        print 'As X, neural network has a score of {:3d}-{:3d}-{:3d} vs {}-depth minimax'.format(test_result['wins'],\n",
    "                                                               test_result['ties'],test_result['losses'],validation_str)\n",
    "\n",
    "        test_result = {'wins':0,'ties':0,'losses':0}\n",
    "\n",
    "        for j in range(100):\n",
    "            result = cccc.play(alpha_beta(validation_str),nn_ai(value_fun_det,'value'))\n",
    "            if result.winner ==1:\n",
    "                test_result['losses'] +=1\n",
    "            if result.winner == 0:\n",
    "                test_result['ties'] +=1\n",
    "            if result.winner == -1:\n",
    "                test_result['wins'] +=1\n",
    "\n",
    "        print 'As O, neural network has a score of {:3d}-{:3d}-{:3d} vs {}-depth minimax'.format(test_result['wins'],\n",
    "                                                            test_result['ties'],test_result['losses'],validation_str)\n",
    "        print ' '\n",
    "        print 'elapsed time: {:3.3f}s'.format(time.clock()-t0)\n",
    "        print ' '\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "#for epoch in range(10):\n",
    "\n",
    "        \n",
    "#    t_epoch = time.clock()\n",
    "\n",
    "#    result_list = [tourney(output) for i in range(BATCH_SIZE)]\n",
    "#    fitness_list = [fitness(result) for result in result_list]\n",
    "    \n",
    "#    score= f_train(fitness_list)\n",
    "       \n",
    "#    t1=time.clock()-t_epoch\n",
    "    \n",
    "#    print('Epoch {}, duration {:.01f} seconds'.format(\n",
    "#            epoch+1, t1))\n",
    "#    print('Record is: {}-{}-{} with a score of {}'.format(sum([result['wins'] for result in result_list]),\n",
    "#                                                          sum([result['ties'] for result in result_list]),\n",
    "#                                                          sum([result['losses'] for result in result_list]),\n",
    "#                                                          sum([fitness for fitness in fitness_list])))\n",
    "#    print('mean score is {}'.format(score))\n",
    "print('total time for neural network is {:.01f} seconds'.format(time.clock()-t0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "f = open('supervised_connect_four.txt','r')\n",
    "plot_list = f.read().splitlines()\n",
    "plot_list = np.array([[float(x) for x in item[1:-1].split(',')] for item in plot_list])\n",
    "\n",
    "f = open('supervised_tictactoe.txt','r')\n",
    "plot_list2 = f.read().splitlines()\n",
    "plot_list2 = np.array([[float(x) for x in item[1:-1].split(',')] for item in plot_list2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,9))\n",
    "plt.plot(plot_list[:,0],plot_list[:,1],color = 'blue',linewidth=3)\n",
    "plt.plot(plot_list[:,0],plot_list[:,2],color = 'green')\n",
    "plt.plot(50*plot_list2[:,0]+plot_list2[:,1]-900,plot_list2[:,2],color = 'red')\n",
    "plt.title('Deep Learning',fontsize=32)\n",
    "plt.xlabel('Epoch',fontsize=24)\n",
    "plt.ylabel('Mean Squared Error',fontsize = 24)\n",
    "plt.xticks(fontsize = 20)\n",
    "plt.yticks(fontsize = 20)\n",
    "ax = plt.subplot(111)    \n",
    "ax.spines[\"top\"].set_visible(False)    \n",
    "# ax.spines[\"bottom\"].set_visible(False)    \n",
    "ax.spines[\"right\"].set_visible(False)    \n",
    "# ax.spines[\"left\"].set_visible(False)    \n",
    "  \n",
    "# Ensure that the axis ticks only show up on the bottom and left of the plot.    \n",
    "# Ticks on the right and top of the plot are generally unnecessary chartjunk.    \n",
    "ax.get_xaxis().tick_bottom()    \n",
    "ax.get_yaxis().tick_left()\n",
    "plt.text(410, 0.61,\"Supervised learning with connect four - loss\",fontsize=20,color='blue')\n",
    "plt.text(410, 0.56,\"Supervised learning with connect four - validation\",fontsize=20,color='green')\n",
    "plt.text(410, 0.51,\"Reinforcement learning with tic tac toe - loss\",fontsize=20,color='red')\n",
    "plt.savefig('supervised_c4.png')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('TD_connect_four.txt','r')\n",
    "plot_list = f.read().splitlines()\n",
    "plot_list = np.array([[float(x) for x in item[1:-1].split(',')] for item in plot_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(500*plot_list[:,0]+plot_list[:,1],plot_list[:,2])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('supervised_tictactoe.txt','r')\n",
    "plot_list = f.read().splitlines()\n",
    "plot_list = np.array([[float(x) for x in item[1:-1].split(',')] for item in plot_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(50*plot_list[:,0]+plot_list[:,1],plot_list[:,2],color = 'red')\n",
    "plt.title('Reinforcement Learning for Tic Tac Toe',fontsize=16)\n",
    "plt.xlabel('Epoch',fontsize=16)\n",
    "plt.ylabel('Mean Squared Error',fontsize = 16)\n",
    "plt.xticks(fontsize = 14)\n",
    "plt.yticks(fontsize = 14)\n",
    "ax = plt.subplot(111)    \n",
    "ax.spines[\"top\"].set_visible(False)    \n",
    "# ax.spines[\"bottom\"].set_visible(False)    \n",
    "ax.spines[\"right\"].set_visible(False)    \n",
    "# ax.spines[\"left\"].set_visible(False)    \n",
    "ax.get_xaxis().tick_bottom()    \n",
    "ax.get_yaxis().tick_left()\n",
    "plt.savefig('reinforcement_ttt.png')\n",
    "plt.show\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
