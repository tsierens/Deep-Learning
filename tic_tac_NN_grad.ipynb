{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lasagne\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import tic_tac_toe as ttt\n",
    "import random\n",
    "import time\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#initialization\n",
    "\n",
    "generations = []\n",
    "\n",
    "hidden_units = 36\n",
    "\n",
    "\n",
    "\n",
    "value_in = lasagne.layers.InputLayer(shape=(None,9))\n",
    "\n",
    "#l_drop1 = lasagne.layers.DropoutLayer(l_shape,p=0.2)\n",
    "\n",
    "value_hid1 = lasagne.layers.DenseLayer(value_in, num_units=hidden_units,W=lasagne.init.GlorotUniform(),\n",
    "                                          nonlinearity = lasagne.nonlinearities.tanh)\n",
    "value_hid2 = lasagne.layers.DenseLayer(value_hid1, num_units=hidden_units,W=lasagne.init.GlorotUniform(),\n",
    "                                          nonlinearity = lasagne.nonlinearities.tanh)\n",
    "\n",
    "value_hid3 = lasagne.layers.DenseLayer(value_hid2, num_units=hidden_units,W=lasagne.init.GlorotUniform(),\n",
    "                                          nonlinearity = lasagne.nonlinearities.tanh)\n",
    "\n",
    "value_drop1 = lasagne.layers.DropoutLayer(value_hid1,p=0.5)\n",
    "#l_drop2 = lasagne.layers.DropoutLayer(l_hid1,p=0.5)\n",
    "\n",
    "value_out = lasagne.layers.DenseLayer(value_drop1,\n",
    "                                  num_units=1, nonlinearity = lasagne.nonlinearities.tanh)\n",
    "\n",
    "\n",
    "\n",
    "policy_in = lasagne.layers.InputLayer(shape=(1,9))\n",
    "\n",
    "#l_drop1 = lasagne.layers.DropoutLayer(l_shape,p=0.2)\n",
    "\n",
    "policy_hid1 = lasagne.layers.DenseLayer(policy_in, num_units=hidden_units,W=lasagne.init.GlorotUniform(),\n",
    "                                          nonlinearity = lasagne.nonlinearities.rectify)\n",
    "\n",
    "#l_drop2 = lasagne.layers.DropoutLayer(l_hid1,p=0.5)\n",
    "\n",
    "policy_out = lasagne.layers.DenseLayer(policy_hid1,\n",
    "                                  num_units=9,\n",
    "                                  nonlinearity=lasagne.nonlinearities.softmax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def policy_move(board,active_turn,output_fun,exploration):\n",
    "    board = board.reshape((1,9))\n",
    "    X_sym = theano.tensor.matrix()\n",
    "    y_sym = theano.tensor.ivector()\n",
    "\n",
    "    player_dict = {'X':1, 'O':-1}\n",
    "    dummy_board = player_dict[active_turn] * board[:] #make 1s good and -1s bad\n",
    "    \n",
    "\n",
    "    move_weights = output_fun(dummy_board)\n",
    "    move_weights = move_weights.reshape(9)\n",
    "\n",
    "    options = ttt.available_moves(dummy_board)\n",
    "    \n",
    "    if exploration > random.random():\n",
    "        move = random.choice(options) \n",
    "    else:\n",
    "        available_move_weights = np.array([move_weights[i] for i in options])\n",
    "\n",
    "        move = options[available_move_weights.argmax(-1)]\n",
    "    \n",
    "    return move+1\n",
    "\n",
    "def value_move(board,active_turn,output_fun,exploration):\n",
    "    board = board.reshape((1,9))\n",
    "    X_sym = theano.tensor.matrix()\n",
    "    y_sym = theano.tensor.ivector()\n",
    "\n",
    "    player_dict = {'X':1, 'O':-1}\n",
    "\n",
    "    dummy_board = player_dict[active_turn] * board[:]\n",
    "    options = ttt.available_moves(dummy_board)\n",
    "    \n",
    "    \n",
    "    if exploration > random.random():\n",
    "        move = random.choice(options) \n",
    "    else:\n",
    "        move_values = np.zeros(9)\n",
    "        for move in options:\n",
    "            dummy_board = player_dict[active_turn] * board[:]\n",
    "            dummy_board[0][move] = 1\n",
    "            move_values[move] = -1 * output_fun(-1* dummy_board)\n",
    "        \n",
    "\n",
    "        available_move_values = np.array([move_values[move] for move in options])\n",
    "        \n",
    "        move = options[available_move_values.argmax(-1)]\n",
    "    return move + 1\n",
    "    \n",
    "\n",
    "    \n",
    "class nn_ai:\n",
    "    \n",
    "    def __init__(self,output_fun, net = 'policy',exploration = 0):\n",
    "        self.output_fun = output_fun\n",
    "        self.exploration = exploration\n",
    "        self.net = net\n",
    "    \n",
    "    def make_move(self,board,active_turn):\n",
    "        if self.net == 'policy':\n",
    "            move = policy_move(board,active_turn,self.output_fun,self.exploration)\n",
    "        if self.net == 'value':\n",
    "            move = value_move(board,active_turn,self.output_fun,self.exploration)\n",
    "        return move\n",
    "\n",
    "def alpha_beta_move(board,active_turn,depth,alpha = 2):\n",
    "    swap_dict = {'X':'O','O':'X'}\n",
    "    dummy_board = np.arange(9)\n",
    "    dummy_board[:] = board[:]\n",
    "    options = ttt.available_moves(board)\n",
    "    random.shuffle(options)\n",
    "    player_dict = {'X':1, 'O':-1}\n",
    "    if len(options) == 1:\n",
    "        dummy_board[options[0]] = player_dict[active_turn]\n",
    "        if ttt.winner(dummy_board):\n",
    "            return (1,options[0]+1)\n",
    "        else:\n",
    "            return (0,options[0]+1)\n",
    "    if depth ==0:\n",
    "        return (0, options[np.random.randint(len(options))]+1)\n",
    "\n",
    "    best_value = -2\n",
    "    candidate_move = None\n",
    "    for x in options:\n",
    "        dummy_board[x] = player_dict[active_turn]\n",
    "        if ttt.winner(dummy_board):\n",
    "            return (1, x+1)\n",
    "        (opp_value,opp_move) = alpha_beta_move(dummy_board,swap_dict[active_turn],depth-1,-best_value)\n",
    "        if -opp_value > best_value:\n",
    "            candidate_move = x+1\n",
    "            best_value = -opp_value\n",
    "        if -opp_value >= alpha:\n",
    "            #print (options, x, best_value, alpha)\n",
    "            break\n",
    "        dummy_board[x] = board[x]\n",
    "\n",
    "    return (best_value, candidate_move)\n",
    "\n",
    "class alpha_beta:\n",
    "    def __init__(self,depth):\n",
    "        self.depth = depth\n",
    "    def make_move(self,board,active_turn):\n",
    "        #print (board,active_turn,self.depth)\n",
    "        return alpha_beta_move(board,active_turn,self.depth)[1]\n",
    "\n",
    "\n",
    "def tourney(output,games = 50,depth = 0):\n",
    "    tourney_results = {'wins' : 0, 'ties' : 0, 'losses' : 0}\n",
    "    for _ in range(games):\n",
    "        results = ttt.play(nn_ai(output),alpha_beta(depth))\n",
    "        if results.winner ==  1:\n",
    "            tourney_results['wins'] +=1\n",
    "        if results.winner ==  0:\n",
    "            tourney_results['ties'] +=1\n",
    "        if results.winner == -1:\n",
    "            tourney_results['losses'] +=1\n",
    "\n",
    "        results = ttt.play(alpha_beta(depth),nn_ai(output))\n",
    "        if results.winner == -1:\n",
    "            tourney_results['wins'] +=1\n",
    "        if results.winner ==  0:\n",
    "            tourney_results['ties'] +=1\n",
    "        if results.winner ==  1:\n",
    "            tourney_results['losses'] +=1\n",
    "    return tourney_results\n",
    "\n",
    "def fitness(score):\n",
    "    return (1.1*score['wins'] + score['ties'])\n",
    "\n",
    "def get_inputs(log):\n",
    "    boards = []\n",
    "    piece = 1\n",
    "    board = np.zeros(9)\n",
    "    boards.append(np.copy(board))\n",
    "    for move in log:\n",
    "        board[move-1] = piece\n",
    "        piece = -piece\n",
    "        boards.append(np.copy(board))\n",
    "    return boards\n",
    "        \n",
    "def get_max_future(future_board,value_fun):\n",
    "    options = ttt.available_moves(future_board)\n",
    "    dummy_board = np.copy(future_board)\n",
    "    move_values = np.zeros(9)\n",
    "    for move in options:\n",
    "        dummy_board = np.copy(future_board)\n",
    "        dummy_board[move] = -1\n",
    "        dummy_board = dummy_board.reshape(1,9)\n",
    "        if ttt.winner(dummy_board):\n",
    "            move_values[move] = ttt.winner(dummy_board)\n",
    "        else:\n",
    "            move_values[move] = value_fun(dummy_board)\n",
    "    \n",
    "    available_move_values = np.array([move_values[move] for move in options])\n",
    "    dummy_board = np.copy(future_board)\n",
    "    options_index = np.argmin(available_move_values)\n",
    "    dummy_board[options[options_index]] = -1\n",
    "    return np.amin(available_move_values), dummy_board\n",
    "\n",
    "def random_move(board,turn):\n",
    "    options = ttt.available_moves(board)\n",
    "    move = random.choice(options)\n",
    "    dummy_board = np.copy(board)\n",
    "    dummy_board[move] = turn\n",
    "    return dummy_board\n",
    "        \n",
    "def random_game(board,turn):\n",
    "    dummy_board = np.copy(board)\n",
    "    while not (ttt.is_winner(dummy_board) or ttt.is_full(dummy_board)):\n",
    "        dummy_board = random_move(dummy_board,turn)\n",
    "        turn = -1*turn\n",
    "    return ttt.winner(dummy_board)\n",
    "    \n",
    "    \n",
    "    \n",
    "def monte_carlo_reward(board,trials = 1000):\n",
    "    reward = 0\n",
    "    for _ in range(trials):\n",
    "        reward += random_game(board,1)\n",
    "    return float(reward) / float(trials)\n",
    "\n",
    "def next_board(board,move,player):\n",
    "    dummy_board = np.copy(board)\n",
    "    dummy_board[move] = player\n",
    "    return dummy_board\n",
    "\n",
    "def game_over(board):\n",
    "    return ttt.winner(board) or ttt.is_full(board)\n",
    "    \n",
    "    \n",
    "    \n",
    "def mc_step(branch,results,epsilon, cutoff = 10000):\n",
    "    dummy_board = np.copy(branch[-1])\n",
    "    #To help convergence we will randomly drop stored values\n",
    "\n",
    "    #if random.random() < 1/float(cutoff):\n",
    "    #    results[tuple(dummy_board)] =  {'result':0,'plays':0}     \n",
    "\n",
    "    \n",
    "    if not results.get(tuple(dummy_board)):\n",
    "        results[tuple(dummy_board)] = {'result':0,'plays':0}\n",
    "        \n",
    "    board_plays = results[tuple(dummy_board)]['plays']\n",
    "    board_result = results[tuple(dummy_board)]['result']\n",
    "    \n",
    "    if game_over(dummy_board):\n",
    "        result = ttt.winner(dummy_board)\n",
    "        \n",
    "    elif board_plays> cutoff:\n",
    "        result = results[tuple(dummy_board)]['result'] / float(results[tuple(dummy_board)]['plays'])\n",
    "        \n",
    "    else: \n",
    "        options = ttt.available_moves(dummy_board)\n",
    "        future_boards = [next_board(dummy_board,move,1) for move in options]\n",
    "        if all(results.get(tuple(-1 * b)) for b in future_boards):\n",
    "            if epsilon(board_plays) > random.random():\n",
    "                dummy_board = random.choice(future_boards)\n",
    "            else:\n",
    "                dummy_board = min(future_boards,key = lambda x :\n",
    "                                  results[tuple(-1 * x)]['result'] / float(results[tuple(-1 * x)]['plays'])) \n",
    "        \n",
    "        else: \n",
    "            dummy_board = random.choice(future_boards)\n",
    "            \n",
    "        branch.append(-1 * np.copy(dummy_board))\n",
    "        result , _ = mc_step(branch,results,epsilon,cutoff)\n",
    "        result = -1 * result\n",
    "    \n",
    "    return result , branch\n",
    "\n",
    "def monte_carlo_mod(board,results,epsilon,duration = 1, player = 1,cutoff = 10000):\n",
    "    #To help convergence we will randomly drop stored values\n",
    "\n",
    "    #if random.random() < 1/float(cutoff):\n",
    "    #    results[tuple(board)] =  {'result':0,'plays':0}     \n",
    "        \n",
    "    t0 = time.clock()\n",
    "    if not results.get(tuple(board)):\n",
    "        results[tuple(board)] = {'result':0,'plays':0}\n",
    "    while (time.clock() - t0 < duration and results[tuple(board)]['plays'] < cutoff):\n",
    "        branch = [player * np.copy(board)]\n",
    "        result , branch = mc_step(branch,results,epsilon,cutoff)\n",
    "    \n",
    "        for i, b in enumerate(branch):\n",
    "            results[tuple(b)]['plays'] +=1\n",
    "            results[tuple(b)]['result'] += (-1) ** i * result\n",
    "    return results[tuple(board)]['result'] / float(results[tuple(board)]['plays'])\n",
    "\n",
    "\n",
    "    \n",
    "def monte_carlo(board,epsilon = 0.5,duration = 1,player=1):\n",
    "    plays = {}\n",
    "    results = {}\n",
    "    t0 = time.clock()\n",
    "    plays[tuple(board)] = 0\n",
    "    results[tuple(board)]=0\n",
    "    \n",
    "    while time.clock()-t0 < duration:\n",
    "        current_player = player\n",
    "        dummy_board = np.copy(board)\n",
    "        branch = [(np.copy(dummy_board),current_player)]   \n",
    "\n",
    "        \n",
    "        while not game_over(dummy_board):\n",
    "            options = ttt.available_moves(dummy_board)\n",
    "            future_boards = [next_board(dummy_board,move,current_player) for move in options]\n",
    "            \n",
    "            if all(plays.get(tuple(b)) for b in future_boards):\n",
    "                if random.random() > epsilon:\n",
    "                    dummy_board = random.choice(future_boards)\n",
    "                else:\n",
    "                    #min here because you are maximizing over future boards, which the results are given in terms of the\n",
    "                    #current player, i.e. the other player.\n",
    "                    dummy_board = min(future_boards,key = lambda x : results[tuple(x)] / float(plays[tuple(x)])) \n",
    "                    \n",
    "            \n",
    "            else:\n",
    "                dummy_board = random.choice(future_boards)\n",
    "                plays[tuple(dummy_board)] = 0\n",
    "                results[tuple(dummy_board)]=0\n",
    "            current_player *= -1    \n",
    "            branch.append((np.copy(dummy_board),current_player))\n",
    "        \n",
    "\n",
    "        for b,p in branch:\n",
    "            plays[tuple(b)] +=1\n",
    "            results[tuple(b)] += p * ttt.winner(dummy_board)\n",
    "            \n",
    "    return results[tuple(board)] / float(plays[tuple(board)])\n",
    "            \n",
    "       \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_sym = T.matrix()\n",
    "y_sym = T.matrix()\n",
    "s_sym = T.scalar()\n",
    "z_sym = T.dscalar()\n",
    "input_history = []\n",
    "output_history = []\n",
    "results = {}\n",
    "move_history = []\n",
    "output = lasagne.layers.get_output(value_out,X_sym)\n",
    "output_det = lasagne.layers.get_output(value_out,X_sym,deterministic=True)\n",
    "value_fun = theano.function([X_sym],output)\n",
    "value_fun_det = theano.function([X_sym],output_det)\n",
    "params = lasagne.layers.get_all_params(value_out)\n",
    "objective = T.mean(lasagne.objectives.squared_error(output,y_sym))\n",
    "grad = T.grad(objective,params)\n",
    "exploration = 1\n",
    "future_discount = 0\n",
    "\n",
    "def epsilon(N):\n",
    "    return 1 - 1. / (float(N) / 100 +1)\n",
    "\n",
    "\n",
    "\n",
    "#flush training sets\n",
    "input_history = []\n",
    "output_history = []\n",
    "results = {}\n",
    "move_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "batches_per_step = 50\n",
    "training_per_step = 10\n",
    "train_duration = 10000\n",
    "exploration = 1\n",
    "exploration_min = 0.05\n",
    "exploration_max = 0.95\n",
    "future_discount = 0.05\n",
    "minimax_str = 0\n",
    "validation_str = 6\n",
    "monte_carlo_duration = 0\n",
    "print_freq = batches_per_step-1\n",
    "valid_freq = 5\n",
    "learning_speed = 0.001\n",
    "updates = lasagne.updates.nesterov_momentum(grad, params, learning_rate=learning_speed,momentum = 0.9)\n",
    "#updates = lasagne.updates.sgd(grad, params, learning_rate=learning_speed)\n",
    "train_net = theano.function([X_sym, y_sym], objective, updates=updates)\n",
    "temporal_index = 0.8\n",
    "objective_total = 1000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning step     0, training size   78, step duration: 0.202s\n",
      "Creating Targets for 841 data points\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t0 = time.clock()\n",
    "epoch_objective_list = []\n",
    "\n",
    "#gonna have it play against random.\n",
    "#might make it play against itself\n",
    "\n",
    "for epoch in range(train_duration):\n",
    "    param_values = lasagne.layers.get_all_param_values(value_out)\n",
    "    \n",
    "    minimum_data = sum([param_values[i].size for i,_ in enumerate(param_values)])\n",
    "    t1 = time.clock()\n",
    "    for _ in range(training_per_step):\n",
    "        exploration = min(exploration_max ,\n",
    "                      exploration*0.99 + 0.01 *(min(exploration_max-exploration_min,objective_total) + exploration_min))\n",
    "        future_discount = future_discount*0.99 + 0.01*(1 - min(1,objective_total))\n",
    "        future_discount = 1\n",
    "        result_X = ttt.play(nn_ai(value_fun,net = 'value',exploration = exploration),alpha_beta(minimax_str))\n",
    "        board_list =get_inputs(result_X.log)\n",
    "        game_length = len(result_X.log)\n",
    "        input_list = [board_list[2*i] for i in range((game_length+1)/2)]\n",
    "        output_list = [board_list[2*i+1] for i in range((game_length+1)/2)]\n",
    "        move_list = [result_X.log[2*i] for i in range((game_length+1)/2)]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # [monte_carlo_mod(-1 * board_list[2*i+1],results,epsilon =\n",
    "        #epsilon,duration = monte_carlo_duration)  for i in range((game_length+1)/2)]\n",
    "    #reward_list = [-1*monte_carlo(-1*board_list[2*i+1],epsilon=epsilon,\n",
    "    #                              duration = monte_carlo_duration) for i in range((game_length+1)/2)]\n",
    "#    reward_list = [ttt.winner(board_list[2*i+1]) for i in range((game_length+1)/2)]\n",
    "    \n",
    "        input_history = input_history+ input_list\n",
    "        output_history = output_history+output_list\n",
    "        move_history = move_history+move_list\n",
    "    #reward_history = reward_history + reward_list\n",
    "\n",
    "\n",
    "        result_O = ttt.play(alpha_beta(minimax_str),nn_ai(value_fun,net = 'value', exploration = exploration))\n",
    "        board_list = get_inputs(result_O.log)\n",
    "        game_length = len(result_O.log)\n",
    "        input_list = [-1*board_list[2*i+1] for i in range(game_length/2)]\n",
    "        output_list = [-1*board_list[2*i+2] for i in range(game_length/2)]\n",
    "        move_list = [result_O.log[2*i+1] for i in range(game_length/2)]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #[monte_carlo_mod(1 * board_list[2*i+2],results,epsilon \n",
    "        #= epsilon,duration = monte_carlo_duration)  for i in range((game_length)/2)]\n",
    "    #reward_list =  [-1*monte_carlo(board_list[2*i+2],epsilon=epsilon,\n",
    "     #                             duration = monte_carlo_duration) for i in range((game_length)/2)]\n",
    "#    reward_list = [-1 * ttt.winner(board_list[2*i+2]) for i in range((game_length)/2)]\n",
    "\n",
    "    \n",
    "        input_history = input_history+ input_list\n",
    "        output_history = output_history+output_list\n",
    "        move_history = move_history+move_list\n",
    "    #reward_history = reward_history + reward_list\n",
    "#    for _ in range(len(input_list)):\n",
    "#        reward_history.append(reward)\n",
    "  \n",
    "\n",
    "    \n",
    "    if len(input_history) > 2*minimum_data:\n",
    "        target_history = np.zeros(len(output_history))\n",
    "        print 'Creating Targets for {} data points'.format(len(output_history))\n",
    "        print '\\n'\n",
    "        t3 = time.clock()\n",
    "        for i,item in enumerate(output_history):\n",
    "            output_state = np.copy(output_history[i])\n",
    "            if ttt.winner(output_state) or ttt.is_full(output_state):\n",
    "                target_history[i] = ttt.winner(output_state)\n",
    "            else:\n",
    "            #minus because the future term is in terms of the valuation for the player, and we need a target for the \n",
    "            #opponent\n",
    "            #    targets[i] = (1-future_discount) * reward_state + future_discount * get_max_future(\n",
    "            #output_state,value_fun)\n",
    "            #targets = np.array(targets).reshape(BATCH_SIZE,1)\n",
    "\n",
    "                #temporal difference method\n",
    "                target_history[i]= 0\n",
    "                current_state = np.copy(output_state)\n",
    "\n",
    "                depth = 0\n",
    "                player = 1\n",
    "\n",
    "                while not game_over(np.copy(current_state)):\n",
    "                    current_value , next_state= get_max_future(current_state,value_fun)\n",
    "\n",
    "                    #get_max_future calculates the min future for other player moving next\n",
    "                    # so the negative player is going to want to reverse it\n",
    "                    current_value = player * current_value\n",
    "                    depth +=1\n",
    "                    target_history[i] += (temporal_index**(depth-1))* (1-temporal_index) *current_value\n",
    "                    current_state = -1 * np.copy(next_state)\n",
    "                    player *= -1\n",
    "\n",
    "                target_history[i] += temporal_index**depth * player* ttt.winner(current_state)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #we reverse the target because we are evaulating the opponenet's position\n",
    "            target_history[i] = -1 * target_history[i]\n",
    "        print 'Time to create targets: {}s'.format(time.clock()-t3)\n",
    "        print '\\n'\n",
    "        for j in range(batches_per_step):\n",
    "            t2=time.clock()\n",
    "            targets = np.zeros((BATCH_SIZE,1))\n",
    "            training = np.zeros((BATCH_SIZE,9))\n",
    "            index_pool = range(len(input_history))\n",
    "            random.shuffle(index_pool)\n",
    "            objectives = []\n",
    "            \n",
    "            while len(index_pool) > BATCH_SIZE:\n",
    "                sample_indices = [index_pool.pop() for _ in xrange(BATCH_SIZE)]\n",
    "                #Should try to use generators\n",
    "                \n",
    "\n",
    "                \n",
    "            \n",
    "            \n",
    "                for k in range(BATCH_SIZE):\n",
    "                    #train it on output_history evaluated by the opponent\n",
    "                    training[k] = (-1*np.copy(output_history[sample_indices[k]]))\n",
    "                    targets[k] = target_history[sample_indices[k]]\n",
    "                    #training[i] = training[i].reshape(1,9)\n",
    "\n",
    "        #               reward_state = reward_history[sample_indices[i]]\n",
    "                    #move_state = move_history[sample_indices[i]]\n",
    "                    #reward_state= -1 * results[tuple(-1 * output_state)]['result'] / float(\n",
    "                    #                   results[tuple(-1 * output_state)]['plays'])\n",
    "\n",
    "                    #needs to evaluate to the result of the opponent's board\n",
    "                    #If move is a winning move, reward_state will evaluate to 1. Since the board is reversed, this is the\n",
    "                    #reverse evaluation\n",
    "\n",
    "\n",
    "\n",
    "                objectives.append(train_net(training,targets))\n",
    "                #break to only do 1 batch per run.\n",
    "            objective_total = np.mean(objectives)\n",
    "            if j%print_freq ==0:\n",
    "                print (('Epoch {:5d}, Pass number {}, objective: {:0.5f}, exploration: {:0.2f}, '+\n",
    "                       'step duration: {:1.3f}s').format(\n",
    "                       epoch,j+1,float(objective_total),exploration,time.clock()-t2))\n",
    "                \n",
    "            epoch_objective_list.append([epoch,j+1,objective_total])\n",
    "        \n",
    "        print 'epoch duration {:2.2f}s'.format(time.clock()-t1)\n",
    "        if epoch%valid_freq==0:\n",
    "            print ' '\n",
    "            print ' '\n",
    "            test_result = {'wins':0,'ties':0,'losses':0}\n",
    "            for j in range(100):\n",
    "                result = ttt.play(nn_ai(value_fun_det,'value'),alpha_beta(validation_str))\n",
    "                if result.winner ==1:\n",
    "                    test_result['wins'] +=1\n",
    "                if result.winner == 0:\n",
    "                    test_result['ties'] +=1\n",
    "                if result.winner == -1:\n",
    "                    test_result['losses'] +=1\n",
    "\n",
    "\n",
    "            print 'As X, neural network has a score of {:3d}-{:3d}-{:3d} vs {}-depth minimax'.format(test_result['wins'],\n",
    "                                                                   test_result['ties'],test_result['losses'],validation_str)\n",
    "    \n",
    "            test_result = {'wins':0,'ties':0,'losses':0}\n",
    "\n",
    "            for j in range(100):\n",
    "                result = ttt.play(alpha_beta(validation_str),nn_ai(value_fun_det,'value'))\n",
    "                if result.winner ==1:\n",
    "                    test_result['losses'] +=1\n",
    "                if result.winner == 0:\n",
    "                    test_result['ties'] +=1\n",
    "                if result.winner == -1:\n",
    "                    test_result['wins'] +=1\n",
    "\n",
    "            print 'As O, neural network has a score of {:3d}-{:3d}-{:3d} vs {}-depth minimax'.format(test_result['wins'],\n",
    "                                                                test_result['ties'],test_result['losses'],validation_str)\n",
    "            print ' '\n",
    "            print 'elapsed time: {:3.3f}s'.format(time.clock()-t0)\n",
    "            print ' '\n",
    "    else:\n",
    "        if epoch%print_freq==0:\n",
    "            print ('Learning step {:5d}, training size {:4d}, step duration: {:1.3f}s'.format(\n",
    "                    epoch,len(input_history),time.clock()-t1))\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "#for epoch in range(10):\n",
    "\n",
    "        \n",
    "#    t_epoch = time.clock()\n",
    "\n",
    "#    result_list = [tourney(output) for i in range(BATCH_SIZE)]\n",
    "#    fitness_list = [fitness(result) for result in result_list]\n",
    "    \n",
    "#    score= f_train(fitness_list)\n",
    "       \n",
    "#    t1=time.clock()-t_epoch\n",
    "    \n",
    "#    print('Epoch {}, duration {:.01f} seconds'.format(\n",
    "#            epoch+1, t1))\n",
    "#    print('Record is: {}-{}-{} with a score of {}'.format(sum([result['wins'] for result in result_list]),\n",
    "#                                                          sum([result['ties'] for result in result_list]),\n",
    "#                                                          sum([result['losses'] for result in result_list]),\n",
    "#                                                          sum([fitness for fitness in fitness_list])))\n",
    "#    print('mean score is {}'.format(score))\n",
    "print('total time for neural network is {:.01f} seconds'.format(time.clock()-t0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As X, neural network has a score of   1- 99-  0 vs 6-depth minimax\n",
      "As O, neural network has a score of   0- 98-  2 vs 6-depth minimax\n",
      " \n",
      "elapsed time: 0.763s\n",
      " \n"
     ]
    }
   ],
   "source": [
    "test_result = {'wins':0,'ties':0,'losses':0}\n",
    "minimax_str = 6\n",
    "player_1 = nn_ai(value_fun,'value')\n",
    "#player_2 = alpha_beta(minimax_str)\n",
    "player_2 = player_1\n",
    "t0 = time.clock()\n",
    "for j in range(100):\n",
    "    result = ttt.play(player_1,player_2)\n",
    "    if result.winner ==1:\n",
    "        test_result['wins'] +=1\n",
    "    if result.winner == 0:\n",
    "        test_result['ties'] +=1\n",
    "    if result.winner == -1:\n",
    "        test_result['losses'] +=1\n",
    "\n",
    "\n",
    "print 'As X, neural network has a score of {:3d}-{:3d}-{:3d} vs {}-depth minimax'.format(test_result['wins'],\n",
    "                                                       test_result['ties'],test_result['losses'],minimax_str)\n",
    "\n",
    "test_result = {'wins':0,'ties':0,'losses':0}\n",
    "\n",
    "for j in range(100):\n",
    "    result = ttt.play(player_2,player_1)\n",
    "    if result.winner ==1:\n",
    "        test_result['losses'] +=1\n",
    "    if result.winner == 0:\n",
    "        test_result['ties'] +=1\n",
    "    if result.winner == -1:\n",
    "        test_result['wins'] +=1\n",
    "\n",
    "print 'As O, neural network has a score of {:3d}-{:3d}-{:3d} vs {}-depth minimax'.format(test_result['wins'],\n",
    "                                                    test_result['ties'],test_result['losses'],minimax_str)\n",
    "print ' '\n",
    "print 'elapsed time: {:3.3f}s'.format(time.clock()-t0)\n",
    "print ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save the chromes\n",
    "np.savez('TD_ttt_nn',lasagne.layers.get_all_param_values(value_out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9L, 64L)\n",
      "(64L,)\n",
      "(64L, 64L)\n",
      "(64L,)\n",
      "(64L, 64L)\n",
      "(64L,)\n",
      "(64L, 1L)\n",
      "(1L,)\n"
     ]
    }
   ],
   "source": [
    "#load the chromes\n",
    "\n",
    "loaded_param = list(np.load('TD_ttt_nn.npz')['arr_0'])\n",
    "for item in loaded_param:\n",
    "    print item.shape\n",
    "lasagne.layers.set_all_param_values(value_out, loaded_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(ttt.play(ttt.player(),nn_ai(value_fun,'value')).winner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = ttt.play(nn_ai(value_fun,'value'),ttt.player())\n",
    "print(result.board,result.winner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reward_history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-7b923d731111>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0moutput_history\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput_history\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0moutput_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mmove_history\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmove_history\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mmove_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mreward_history\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward_history\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mreward_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'reward_history' is not defined"
     ]
    }
   ],
   "source": [
    "minimax_str = 6\n",
    "player_1 = nn_ai(value_fun,'value')\n",
    "player_2 = alpha_beta(minimax_str)\n",
    "for epoch in range(100):\n",
    "    t1 = time.clock()\n",
    "    result_X = ttt.play(player_1,player_2)\n",
    "    board_list =get_inputs(result_X.log)\n",
    "    game_length = len(result_X.log)\n",
    "    input_list = [board_list[2*i] for i in range((game_length+1)/2)]\n",
    "    output_list = [board_list[2*i+1] for i in range((game_length+1)/2)]\n",
    "    move_list = [result_X.log[2*i] for i in range((game_length+1)/2)]\n",
    "#    reward_list = [-1*monte_carlo(-1*board_list[2*i+1],epsilon=epsilon,\n",
    "#                                  duration = monte_carlo_duration) for i in range((game_length+1)/2)]\n",
    "    reward_list = [alpha_beta_move(board_list[2*i],'X',7)[0] for i in range((game_length+1)/2)]\n",
    "    \n",
    "    input_history = input_history+ input_list\n",
    "    output_history = output_history+output_list\n",
    "    move_history = move_history+move_list\n",
    "    reward_history = reward_history + reward_list\n",
    "\n",
    "\n",
    "    result_O = ttt.play(player_2,player_1)\n",
    "    board_list = get_inputs(result_O.log)\n",
    "    game_length = len(result_O.log)\n",
    "    input_list = [-1*board_list[2*i+1] for i in range(game_length/2)]\n",
    "    output_list = [-1*board_list[2*i+2] for i in range(game_length/2)]\n",
    "    move_list = [result_O.log[2*i+1] for i in range(game_length/2)]\n",
    "#    reward_list =  [-1*monte_carlo(board_list[2*i+2],epsilon=epsilon,\n",
    "#                                  duration = monte_carlo_duration) for i in range((game_length)/2)]\n",
    "    reward_list = [alpha_beta_move(-1*board_list[2*i+1],'X',7)[0] for i in range((game_length)/2)]\n",
    "\n",
    "    \n",
    "    input_history = input_history+ input_list\n",
    "    output_history = output_history+output_list\n",
    "    move_history = move_history+move_list\n",
    "    reward_history = reward_history + reward_list\n",
    "    if epoch%10 ==0:\n",
    "        print epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "pd.DataFrame.to_csv(pd.DataFrame(input_history),'minimax_input_history.csv')\n",
    "pd.DataFrame.to_csv(pd.DataFrame(output_history),'minimax_output_history.csv')\n",
    "pd.DataFrame.to_csv(pd.DataFrame(move_history),'minimax_move_history.csv')\n",
    "pd.DataFrame.to_csv(pd.DataFrame(reward_history),'minimax_reward_history.csv')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "batches_per_step = 1\n",
    "train_duration = 20000\n",
    "exploration = 1\n",
    "exploration_min = 0.05\n",
    "future_discount = 0\n",
    "minimax_str = 0\n",
    "validation_str = 2\n",
    "epsilon = 0.5\n",
    "monte_carlo_duration = 1\n",
    "objective_total = 1000.0\n",
    "print_freq = 100\n",
    "valid_freq = 2000\n",
    "valid_size = 100\n",
    "learning_speed = 0.01\n",
    "updates = lasagne.updates.nesterov_momentum(grad, params, learning_rate=learning_speed,momentum = 0.9)\n",
    "train_net = theano.function([X_sym, y_sym], objective, updates=updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "input_history = list(pd.DataFrame.from_csv('minimax_input_history.csv').values)\n",
    "output_history = list(pd.DataFrame.from_csv('minimax_output_history.csv').values)\n",
    "move_history = list(pd.DataFrame.from_csv('minimax_move_history.csv').values)\n",
    "reward_history = list(pd.DataFrame.from_csv('minimax_reward_history.csv').values)\n",
    "#reward_history = [item[0] for item in reward_history]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t0 = time.clock()\n",
    "\n",
    "#gonna have it play against random.\n",
    "#might make it play against itself\n",
    "\n",
    "for epoch in range(train_duration):\n",
    "    t1 = time.clock()\n",
    "    if len(input_history) > 2*BATCH_SIZE:\n",
    "        for _ in range(batches_per_step):\n",
    "            targets = [0]*BATCH_SIZE\n",
    "            inputs = np.zeros((BATCH_SIZE,9))\n",
    "            sample_indices = random.sample(range(len(input_history)),BATCH_SIZE)\n",
    "            #Should try to use generators\n",
    "\n",
    "            for i in range(BATCH_SIZE):\n",
    "                #this seems confusing, but the rewards are based on output from the net\n",
    "                inputs[i] = (np.copy(input_history[sample_indices[i]]))\n",
    "#               inputs[i] = inputs[i].reshape(1,9)\n",
    "                output_state = output_history[sample_indices[i]]\n",
    "#               reward_state = reward_history[sample_indices[i]]\n",
    "                move_state = move_history[sample_indices[i]]\n",
    "                reward_state=reward_history[sample_indices[i]]\n",
    "                targets[i] = reward_state\n",
    "               \n",
    "            objective_total = train_net(inputs,targets)\n",
    "        if epoch%print_freq ==0:\n",
    "            print ('Learning step {:5d}, objective: {:0.5f}, step duration: {:1.3f}s'.format(\n",
    "                    epoch,float(objective_total),time.clock()-t1))\n",
    "        \n",
    "        if epoch%valid_freq==0:\n",
    "            print ' '\n",
    "            print ' '\n",
    "            test_result = {'wins':0,'ties':0,'losses':0}\n",
    "            for j in range(valid_size):\n",
    "                result = ttt.play(nn_ai(value_fun,'value'),alpha_beta(validation_str))\n",
    "                if result.winner ==1:\n",
    "                    test_result['wins'] +=1\n",
    "                if result.winner == 0:\n",
    "                    test_result['ties'] +=1\n",
    "                if result.winner == -1:\n",
    "                    test_result['losses'] +=1\n",
    "\n",
    "\n",
    "            print 'As X, neural network has a score of {:3d}-{:3d}-{:3d} vs {}-depth minimax'.format(test_result['wins'],\n",
    "                                                                   test_result['ties'],test_result['losses'],validation_str)\n",
    "    \n",
    "            test_result = {'wins':0,'ties':0,'losses':0}\n",
    "\n",
    "            for j in range(valid_size):\n",
    "                result = ttt.play(alpha_beta(validation_str),nn_ai(value_fun,'value'))\n",
    "                if result.winner ==1:\n",
    "                    test_result['losses'] +=1\n",
    "                if result.winner == 0:\n",
    "                    test_result['ties'] +=1\n",
    "                if result.winner == -1:\n",
    "                    test_result['wins'] +=1\n",
    "\n",
    "            print 'As O, neural network has a score of {:3d}-{:3d}-{:3d} vs {}-depth minimax'.format(test_result['wins'],\n",
    "                                                                        test_result['ties'],test_result['losses'],validation_str)\n",
    "            print ' '\n",
    "            print 'elapsed time: {:3.3f}s'.format(time.clock()-t0)\n",
    "            print ' '\n",
    "    else:\n",
    "        if epoch%print_freq==0:\n",
    "            print ('Learning step {:5d}, training size {:4d}, step duration: {:1.3f}s'.format(\n",
    "                    epoch,len(input_history), time.clock() - t1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#save the chromes\n",
    "np.savez('supervised_single_ttt_nn',lasagne.layers.get_all_param_values(value_out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loaded_param = list(np.load('supervised_single_ttt_nn.npz')['arr_0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i,item in enumerate(output_history):\n",
    "    output_state = np.copy(output_history[i])\n",
    "    if ttt.winner(output_state) or ttt.is_full(output_state):\n",
    "        target_history[i] = ttt.winner(output_state)\n",
    "    else:\n",
    "    #minus because the future term is in terms of the valuation for the player, and we need a target for the \n",
    "    #opponent\n",
    "    #    targets[i] = (1-future_discount) * reward_state + future_discount * get_max_future(\n",
    "    #output_state,value_fun)\n",
    "    #targets = np.array(targets).reshape(BATCH_SIZE,1)\n",
    "\n",
    "        #temporal difference method\n",
    "        target_history[i]= 0\n",
    "        current_state = np.copy(output_state)\n",
    "\n",
    "        depth = 0\n",
    "        player = 1\n",
    "\n",
    "        while not game_over(np.copy(current_state)):\n",
    "            current_value , next_state= get_max_future(current_state,value_fun)\n",
    "\n",
    "            #get_max_future calculates the min future for other player moving next\n",
    "            # so the negative player is going to want to reverse it\n",
    "            current_value = player * current_value\n",
    "            depth +=1\n",
    "            target_history[i] += (temporal_index**(depth-1))* (1-temporal_index) *current_value\n",
    "            current_state = -1 * np.copy(next_state)\n",
    "            player *= -1\n",
    "\n",
    "        target_history[i] += temporal_index**depth * player* ttt.winner(current_state)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #we reverse the target because we are evaulating the opponenet's position\n",
    "    target_history[i] = -1 * target_history[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_state = np.copy(output_history[6551])\n",
    "if ttt.winner(output_state) or ttt.is_full(output_state):\n",
    "    test = ttt.winner(output_state)\n",
    "else:\n",
    "#minus because the future term is in terms of the valuation for the player, and we need a target for the \n",
    "#opponent\n",
    "#    targets[i] = (1-future_discount) * reward_state + future_discount * get_max_future(\n",
    "#output_state,value_fun)\n",
    "#targets = np.array(targets).reshape(BATCH_SIZE,1)\n",
    "\n",
    "    #temporal difference method\n",
    "    test= 0\n",
    "    current_state = np.copy(output_state)\n",
    "\n",
    "    depth = 0\n",
    "    player = 1\n",
    "\n",
    "    while not game_over(np.copy(current_state)):\n",
    "        current_value , next_state= get_max_future(current_state,value_fun)\n",
    "\n",
    "        #get_max_future calculates the min future for other player moving next\n",
    "        # so the negative player is going to want to reverse it\n",
    "        current_value = player * current_value\n",
    "        depth +=1\n",
    "        test += (temporal_index**(depth-1))* (1-temporal_index) *current_value\n",
    "        current_state = -1 * np.copy(next_state)\n",
    "        player *= -1\n",
    "\n",
    "    test += temporal_index**depth * player* ttt.winner(current_state)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#we reverse the target because we are evaulating the opponenet's position\n",
    "test = -1 * test\n",
    "print test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_im(im,num_channels):\n",
    "    for j in range(num_channels):    \n",
    "        plt.subplot(1, num_channels, j+1)\n",
    "        plt.imshow(im[0][j], interpolation='nearest')\n",
    "        plt.axis('off')\n",
    "        \n",
    "input_grid = lasagne.layers.get_output(value_in, X_sym)\n",
    "layer1_grid = lasagne.layers.get_output(value_hid1, X_sym)\n",
    "layer2_grid = lasagne.layers.get_output(value_hid2, X_sym)\n",
    "layer3_grid = lasagne.layers.get_output(value_hid3, X_sym)\n",
    "output_grid = lasagne.layers.get_output(value_out, X_sym)\n",
    "\n",
    "training_index = 130\n",
    "vmin = -1\n",
    "vmax = +1\n",
    "color_map = plt.cm.bwr\n",
    "\n",
    "fig, axes = plt.subplots(1,5,figsize = (20,6))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# plt.setp(axes, xticks=[], xticklabels=[],\n",
    "#         yticks=[],yticklabels=[])\n",
    "\n",
    "# plt.xticks([])\n",
    "\n",
    "# axes.tick_params(\n",
    "#     axis='x',          # changes apply to the x-axis\n",
    "#     which='both',      # both major and minor ticks are affected\n",
    "#     bottom='off',      # ticks along the bottom edge are off\n",
    "#     top='off',         # ticks along the top edge are off\n",
    "#     labelbottom='off') # labels along the bottom edge are off\n",
    "\n",
    "f_filter = theano.function([X_sym], input_grid)\n",
    "im = f_filter(training[training_index:training_index+1])\n",
    "\n",
    "axes[0].matshow(im.reshape((3,3)),cmap=color_map,vmin = vmin,vmax=vmax)\n",
    "\n",
    "axes[0].plot([1.5, 1.5], [-0.75, 2.75], 'black', lw=4)\n",
    "axes[0].plot([0.5, 0.5], [-0.75, 2.75], 'black', lw=4)\n",
    "axes[0].plot([-0.75, 2.75], [1.5, 1.5], 'black', lw=4)\n",
    "axes[0].plot([-0.75, 2.75], [0.5, 0.5], 'black', lw=4)\n",
    "axes[0].set_title('Tic Tac Toe Position',fontsize = 20)\n",
    "\n",
    "f_filter = theano.function([X_sym], layer1_grid)\n",
    "im = f_filter(training[training_index:training_index+1])\n",
    "\n",
    "axes[1].matshow(im.reshape((6,6)),cmap=color_map,vmin = vmin,vmax=vmax)\n",
    "\n",
    "f_filter = theano.function([X_sym], layer2_grid)\n",
    "im = f_filter(training[training_index:training_index+1])\n",
    "\n",
    "axes[2].matshow(im.reshape((6,6)),cmap=color_map,vmin = vmin,vmax=vmax)\n",
    "\n",
    "\n",
    "f_filter = theano.function([X_sym], layer3_grid)\n",
    "im = f_filter(training[training_index:training_index+1])\n",
    "axes[2].set_title('Network Node Activations',fontsize=20)\n",
    "axes[3].matshow(im.reshape((6,6)),cmap=color_map,vmin = vmin,vmax=vmax)\n",
    "\n",
    "\n",
    "f_filter = theano.function([X_sym], output_grid)\n",
    "im = f_filter(training[training_index:training_index+1])\n",
    "\n",
    "axes[4].matshow(im,cmap=color_map,vmin = vmin,vmax=vmax)\n",
    "axes[4].set_title('Predicted Winner',fontsize=20)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "plt.savefig('network_weights.png')\n",
    "plt.show()\n",
    "print im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it is X's turn, please choose a move\n",
      "\n",
      "     |   |              1 | 2 | 3\n",
      "   ----------          -----------\n",
      "     |   |              4 | 5 | 6\n",
      "   ----------          -----------\n",
      "     |   |              7 | 8 | 9\n",
      "    \n",
      "5\n",
      "it is X's turn, please choose a move\n",
      "\n",
      "   O |   |              1 | 2 | 3\n",
      "   ----------          -----------\n",
      "     | X |              4 | 5 | 6\n",
      "   ----------          -----------\n",
      "     |   |              7 | 8 | 9\n",
      "    \n",
      "3\n",
      "it is X's turn, please choose a move\n",
      "\n",
      "   O |   | X            1 | 2 | 3\n",
      "   ----------          -----------\n",
      "     | X |              4 | 5 | 6\n",
      "   ----------          -----------\n",
      "   O |   |              7 | 8 | 9\n",
      "    \n",
      "4\n",
      "it is X's turn, please choose a move\n",
      "\n",
      "   O |   | X            1 | 2 | 3\n",
      "   ----------          -----------\n",
      "   X | X | O            4 | 5 | 6\n",
      "   ----------          -----------\n",
      "   O |   |              7 | 8 | 9\n",
      "    \n",
      "8\n",
      "it is X's turn, please choose a move\n",
      "\n",
      "   O | O | X            1 | 2 | 3\n",
      "   ----------          -----------\n",
      "   X | X | O            4 | 5 | 6\n",
      "   ----------          -----------\n",
      "   O | X |              7 | 8 | 9\n",
      "    \n",
      "9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tic_tac_toe.result at 0x662a860>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttt.play(ttt.player(),alpha_beta(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
